{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # suppress Tensorflow verbose prints\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import spatial\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import random\n",
    "import utils\n",
    "import math\n",
    "import cv2\n",
    "\n",
    "dataset_path = Path.cwd() / 'datasets'\n",
    "test_dataset_path = dataset_path / 'lfw_112x96'\n",
    "\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-d6885b346d0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_test_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_dataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_dataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'flag'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_dataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'flag'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_test_data' is not defined"
     ]
    }
   ],
   "source": [
    "test_data = load_test_data(dataset_path)\n",
    "test_dataframe = pd.DataFrame(test_data)\n",
    "test_dataframe['flag'] = test_dataframe['flag'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CASIA pairs for training\n",
      "LFW pairs for testing\n",
      "Loading train data\n",
      "Loading test data\n",
      "shuffle casia list\n"
     ]
    }
   ],
   "source": [
    "print(\"CASIA pairs for training\")\n",
    "print(\"LFW pairs for testing\")\n",
    "casia_pairs, lfw_pairs = load_data(data_path)\n",
    "\n",
    "print(\"shuffle casia list\")\n",
    "np.random.shuffle(casia_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataframe\n",
      "                    fileL            fileR flag\n",
      "0         1209966/046.jpg  1428874/001.jpg    0\n",
      "1         1500155/371.jpg  0447695/104.jpg    0\n",
      "2         0832314/092.jpg  2803556/035.jpg    0\n",
      "3         0330687/001.jpg  0330687/026.jpg    1\n",
      "4         0215281/096.jpg  0215281/079.jpg    1\n",
      "...                   ...              ...  ...\n",
      "46852987  0000571/204.jpg  0000571/074.jpg    1\n",
      "46852988  0000159/395.jpg  0000159/241.jpg    1\n",
      "46852989  1013003/056.jpg  0714147/021.jpg    0\n",
      "46852990  0386472/242.jpg  0386472/189.jpg    1\n",
      "46852991  2440627/001.jpg  2440627/022.jpg    1\n",
      "\n",
      "[46852992 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "casia_list = casia_pairs.tolist()\n",
    "\n",
    "# convert to pandas dataframe\n",
    "train_dataframe = pd.DataFrame(casia_list) # takes long time\n",
    "train_dataframe['flag'] = train_dataframe['flag'].astype(str)\n",
    "print(\"train dataframe\")\n",
    "print(train_dataframe)\n",
    "\n",
    "del casia_pairs, casia_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dataframe\n",
      "                                                  fileL  \\\n",
      "0                    Abel_Pacheco/Abel_Pacheco_0001.jpg   \n",
      "1                Akhmed_Zakayev/Akhmed_Zakayev_0001.jpg   \n",
      "2                Akhmed_Zakayev/Akhmed_Zakayev_0002.jpg   \n",
      "3                  Amber_Tamblyn/Amber_Tamblyn_0001.jpg   \n",
      "4                Angela_Bassett/Angela_Bassett_0001.jpg   \n",
      "...                                                 ...   \n",
      "5556                     Scott_Wolf/Scott_Wolf_0002.jpg   \n",
      "5557  Sergei_Alexandrovitch_Ordzhonikidze/Sergei_Ale...   \n",
      "5558                     Shane_Loux/Shane_Loux_0001.jpg   \n",
      "5559                 Shawn_Marion/Shawn_Marion_0001.jpg   \n",
      "5560     Slobodan_Milosevic/Slobodan_Milosevic_0002.jpg   \n",
      "\n",
      "                                       fileR flag  \n",
      "0         Abel_Pacheco/Abel_Pacheco_0004.jpg    1  \n",
      "1     Akhmed_Zakayev/Akhmed_Zakayev_0003.jpg    1  \n",
      "2     Akhmed_Zakayev/Akhmed_Zakayev_0003.jpg    1  \n",
      "3       Amber_Tamblyn/Amber_Tamblyn_0002.jpg    1  \n",
      "4     Angela_Bassett/Angela_Bassett_0005.jpg    1  \n",
      "...                                      ...  ...  \n",
      "5556    Troy_Polamalu/Troy_Polamalu_0001.jpg    0  \n",
      "5557      Yolanda_King/Yolanda_King_0001.jpg    0  \n",
      "5558      Val_Ackerman/Val_Ackerman_0001.jpg    0  \n",
      "5559    Shirley_Jones/Shirley_Jones_0001.jpg    0  \n",
      "5560                  Sok_An/Sok_An_0001.jpg    0  \n",
      "\n",
      "[5561 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "lfw_list = lfw_pairs.tolist()\n",
    "\n",
    "# convert to pandas dataframe\n",
    "test_dataframe = pd.DataFrame(lfw_list) # takes long time\n",
    "test_dataframe['flag'] = test_dataframe['flag'].astype(str)\n",
    "print(\"test dataframe\")\n",
    "print(test_dataframe)\n",
    "\n",
    "del lfw_pairs, lfw_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/how-to-load-large-datasets-from-directories-for-deep-learning-with-keras/\n",
    "# https://medium.com/@vijayabhaskar96/tutorial-on-keras-flow-from-dataframe-1fd4493d237c\n",
    "# https://stackoverflow.com/questions/49404993/keras-how-to-use-fit-generator-with-multiple-inputs\n",
    "\n",
    "def preprocess_image(image):\n",
    "    image = (image - 127.5) / 128\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generator(test_dataframe, batch_size):\n",
    "    class_mode_ = \"binary\"\n",
    "    test_datagen = ImageDataGenerator(\n",
    "                             preprocessing_function=preprocess_image) \n",
    "    test_generator_X1 = test_datagen.flow_from_dataframe(\n",
    "                             dataframe=test_dataframe,\n",
    "                             directory=str(test_dataset_path) + \"/\",\n",
    "                             x_col=\"fileL\",\n",
    "                             y_col=\"flag\",\n",
    "                             batch_size=batch_size,\n",
    "                             seed=42,\n",
    "                             shuffle=False,\n",
    "                             class_mode=class_mode_,\n",
    "                             color_mode='rgb',\n",
    "                             target_size=(112,96))\n",
    "\n",
    "    test_generator_X2 = test_datagen.flow_from_dataframe(\n",
    "                             dataframe=test_dataframe,\n",
    "                             directory=str(test_dataset_path) + \"/\",\n",
    "                             x_col=\"fileR\",\n",
    "                             y_col=\"flag\",\n",
    "                             batch_size=batch_size,\n",
    "                             seed=42,\n",
    "                             shuffle=False,\n",
    "                             class_mode=class_mode_,\n",
    "                             color_mode='rgb',\n",
    "                             target_size=(112,96))\n",
    "    while True:\n",
    "        X1i = test_generator_X1.next()\n",
    "        X2i = test_generator_X2.next()\n",
    "        yield [X1i[0], X2i[0]], X1i[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/paperspace/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/paperspace/.local/lib/python3.6/site-packages/tensorflow/python/keras/initializers.py:94: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Conv2D, Add, Activation, PReLU, Dense, Input, ZeroPadding2D, Lambda, GlobalAveragePooling2D, Dot \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.losses import CosineSimilarity\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "# from keras.engine.topology import Layer\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow import keras\n",
    "\n",
    "def conv_3_block(input, filters):\n",
    "    x = ZeroPadding2D(padding=(1, 1))(input)\n",
    "    x = Conv2D(filters, 3, strides=2, padding='valid', kernel_initializer='glorot_uniform')(x)\n",
    "    r1 = PReLU()(x)\n",
    "\n",
    "    x = ZeroPadding2D(padding=(1, 1))(r1)\n",
    "    x = Conv2D(filters, 3, strides=1, padding='valid', kernel_initializer=TruncatedNormal(stddev=0.01))(x)\n",
    "    r2 = PReLU()(x)\n",
    "\n",
    "    x = ZeroPadding2D(padding=(1, 1))(r2)\n",
    "    x = Conv2D(filters, 3, strides=1, padding='valid', kernel_initializer=TruncatedNormal(stddev=0.01))(x)\n",
    "    r3 = PReLU()(x)\n",
    "\n",
    "    x = Add()([r1, r3])\n",
    "    return x \n",
    "\n",
    "def conv_2_block(input, filters):\n",
    "    x = ZeroPadding2D(padding=(1, 1))(input)\n",
    "    x = Conv2D(filters, 3, strides=1, padding='valid', kernel_initializer=TruncatedNormal(stddev=0.01))(x)\n",
    "    x = PReLU()(x)\n",
    "\n",
    "    x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "    x = Conv2D(filters, 3, strides=1, padding='valid', kernel_initializer=TruncatedNormal(stddev=0.01))(x)\n",
    "    x = PReLU()(x)\n",
    "\n",
    "    x = Add()([input, x])\n",
    "    return x\n",
    "\n",
    "def sphereface20(input_shape):\n",
    "    input = Input(shape=input_shape)\n",
    "    x = conv_3_block(input, 64)\n",
    "    x = conv_3_block(x, 128)\n",
    "    x = conv_2_block(x, 128)\n",
    "    x = conv_3_block(x, 256)\n",
    "    x = conv_2_block(x, 256)\n",
    "    x = conv_2_block(x, 256)\n",
    "    x = conv_2_block(x, 256)\n",
    "    x = conv_3_block(x, 512)\n",
    "    \n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(512, kernel_initializer='glorot_uniform')(x)\n",
    "    \n",
    "    model = Model(input, x)\n",
    "    return model\n",
    "\n",
    "# https://keras.io/examples/mnist_siamese/\n",
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n",
    "    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n",
    "\n",
    "# https://stackoverflow.com/questions/51003027/computing-cosine-similarity-between-two-tensors-in-keras\n",
    "def cosine_distance(vests):\n",
    "    x, y = vests\n",
    "    x = K.l2_normalize(x, axis=-1)\n",
    "    y = K.l2_normalize(y, axis=-1)\n",
    "    return -K.mean(x * y, axis=-1, keepdims=True)\n",
    "\n",
    "def cos_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0],1)\n",
    "\n",
    "\n",
    "# network definition\n",
    "input_shape = (112, 96, 3)\n",
    "base_network = sphereface20(input_shape)\n",
    "\n",
    "input_a = Input(shape=input_shape)\n",
    "input_b = Input(shape=input_shape)\n",
    "\n",
    "# because we re-use the same instance `base_network`,\n",
    "# the weights of the network\n",
    "# will be shared across the two branches\n",
    "processed_a = base_network(input_a)\n",
    "processed_b = base_network(input_b)\n",
    "distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([processed_a, processed_b])\n",
    "\n",
    "model = Model([input_a, input_b], distance)\n",
    "\n",
    "# serialize model to JSON\n",
    "folder_path = Path.cwd().parent / 'models' / 'sphereface_20_keras'\n",
    "folder_path.mkdir(parents=True, exist_ok=True)\n",
    "model_path = str(folder_path / 'sphereface_20.json')\n",
    "model_json = base_network.to_json()\n",
    "with open(model_path, \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    return K.mean(K.equal(y_true, K.cast(y_pred < 0.5, y_true.dtype)))\n",
    "\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    '''Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    margin = 1\n",
    "    square_pred = K.square(y_pred)\n",
    "    margin_square = K.square(K.maximum(margin - y_pred, 0))\n",
    "    return K.mean(y_true * square_pred + (1 - y_true) * margin_square)\n",
    "\n",
    "def custom_loss(yTrue,yPred):\n",
    "    return K.sum(K.log(yTrue) - K.log(yPred))\n",
    "\n",
    "# def cosine_similarity(y_true, y_pred):\n",
    "#     y = tf.constant([c1,c2])\n",
    "#     x = K.l2_normalize(y_true, -1)\n",
    "#     y = K.l2_normalize(y_pred, -1)\n",
    "#     s = K.mean(x * y, axis=-1, keepdims=False) * 10\n",
    "#     return s\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    0.0001,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True)\n",
    "# optimizer_ = keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "\n",
    "# rms_ = RMSprop()\n",
    "optimizer_ = RMSprop(learning_rate=0.001, rho=0.9, epsilon=1e-07)\n",
    "# loss_ = custom_loss\n",
    "# loss_ = CosineSimilarity(axis=-1, name='cosine_similarity')\n",
    "\n",
    "model.compile(loss=contrastive_loss, optimizer=optimizer_, metrics=[accuracy])\n",
    "# model.compile(loss=loss_, optimizer=rms_, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Found 46852992 validated image filenames belonging to 2 classes.\n",
      "Found 46852992 validated image filenames belonging to 2 classes.\n",
      "WARNING:tensorflow:From /home/paperspace/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "3124/3125 [============================>.] - ETA: 0s - loss: 20846201.3692 - accuracy: 0.5661Found 5561 validated image filenames belonging to 2 classes.\n",
      "Found 5561 validated image filenames belonging to 2 classes.\n",
      "3125/3125 [==============================] - 1581s 506ms/step - loss: 20839530.5849 - accuracy: 0.5661 - val_loss: 0.2377 - val_accuracy: 0.6040\n",
      "Epoch 2/5\n",
      "3125/3125 [==============================] - 719s 230ms/step - loss: 5.3044 - accuracy: 0.6525 - val_loss: 0.1822 - val_accuracy: 0.7282\n",
      "Epoch 3/5\n",
      "3125/3125 [==============================] - 720s 230ms/step - loss: 0.3899 - accuracy: 0.6974 - val_loss: 0.1751 - val_accuracy: 0.7391\n",
      "Epoch 4/5\n",
      "3125/3125 [==============================] - 724s 232ms/step - loss: 1.1653 - accuracy: 0.7104 - val_loss: 0.1755 - val_accuracy: 0.7501\n",
      "Epoch 5/5\n",
      "3125/3125 [==============================] - 724s 232ms/step - loss: 2.5787 - accuracy: 0.7135 - val_loss: 0.1992 - val_accuracy: 0.6863\n"
     ]
    }
   ],
   "source": [
    "batch_size_ = 32\n",
    "images_per_epoch_ = 100000\n",
    "epochs_ = 5\n",
    "\n",
    "# https://github.com/keras-team/keras/issues/10855\n",
    "hist = model.fit(train_generator(train_dataframe, batch_size_), \n",
    "                 steps_per_epoch = images_per_epoch_ // batch_size_,\n",
    "                 epochs = epochs_,\n",
    "                 validation_data = test_generator(test_dataframe, batch_size_),\n",
    "                 validation_steps = len(test_dataframe) // batch_size_,\n",
    "                 shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Found 46852992 validated image filenames belonging to 2 classes.\n",
      "Found 46852992 validated image filenames belonging to 2 classes.\n",
      "WARNING:tensorflow:From /home/paperspace/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "3124/3125 [============================>.] - ETA: 0s - loss: 0.2405 - accuracy: 0.5942Found 5561 validated image filenames belonging to 2 classes.\n",
      "Found 5561 validated image filenames belonging to 2 classes.\n",
      "lr: 0.01\n",
      "3125/3125 [==============================] - 1541s 493ms/step - loss: 0.2405 - accuracy: 0.5942 - val_loss: 0.2089 - val_accuracy: 0.6671\n",
      "Epoch 2/20\n",
      "3124/3125 [============================>.] - ETA: 0s - loss: 0.2050 - accuracy: 0.6755lr: 0.01\n",
      "3125/3125 [==============================] - 697s 223ms/step - loss: 0.2050 - accuracy: 0.6755 - val_loss: 0.1804 - val_accuracy: 0.7291\n",
      "Epoch 3/20\n",
      "3124/3125 [============================>.] - ETA: 0s - loss: 0.1830 - accuracy: 0.7222lr: 0.01\n",
      "3125/3125 [==============================] - 696s 223ms/step - loss: 0.1830 - accuracy: 0.7222 - val_loss: 0.1686 - val_accuracy: 0.7589\n",
      "Epoch 4/20\n",
      "3124/3125 [============================>.] - ETA: 0s - loss: 0.1710 - accuracy: 0.7454lr: 0.01\n",
      "3125/3125 [==============================] - 697s 223ms/step - loss: 0.1710 - accuracy: 0.7454 - val_loss: 0.1652 - val_accuracy: 0.7611\n",
      "Epoch 5/20\n",
      "3124/3125 [============================>.] - ETA: 0s - loss: 0.1635 - accuracy: 0.7596lr: 0.01\n",
      "3125/3125 [==============================] - 702s 225ms/step - loss: 0.1635 - accuracy: 0.7596 - val_loss: 0.1578 - val_accuracy: 0.7686\n",
      "Epoch 6/20\n",
      "3124/3125 [============================>.] - ETA: 0s - loss: 0.1596 - accuracy: 0.7670lr: 0.01\n",
      "3125/3125 [==============================] - 703s 225ms/step - loss: 0.1596 - accuracy: 0.7670 - val_loss: 0.1559 - val_accuracy: 0.7772\n",
      "Epoch 7/20\n",
      "3124/3125 [============================>.] - ETA: 0s - loss: 0.1541 - accuracy: 0.7782lr: 0.01\n",
      "3125/3125 [==============================] - 702s 225ms/step - loss: 0.1541 - accuracy: 0.7782 - val_loss: 0.1470 - val_accuracy: 0.8003\n",
      "Epoch 8/20\n",
      "3124/3125 [============================>.] - ETA: 0s - loss: 0.1514 - accuracy: 0.7827lr: 0.01\n",
      "3125/3125 [==============================] - 705s 225ms/step - loss: 0.1514 - accuracy: 0.7826 - val_loss: 0.1510 - val_accuracy: 0.7811\n",
      "Epoch 9/20\n",
      "3124/3125 [============================>.] - ETA: 0s - loss: 0.1480 - accuracy: 0.7903lr: 0.01\n",
      "3125/3125 [==============================] - 702s 225ms/step - loss: 0.1480 - accuracy: 0.7903 - val_loss: 0.1431 - val_accuracy: 0.8086\n",
      "Epoch 10/20\n",
      "3124/3125 [============================>.] - ETA: 0s - loss: 0.1463 - accuracy: 0.7927lr: 0.01\n",
      "3125/3125 [==============================] - 702s 225ms/step - loss: 0.1462 - accuracy: 0.7927 - val_loss: 0.1406 - val_accuracy: 0.8045\n",
      "Epoch 11/20\n",
      "3124/3125 [============================>.] - ETA: 0s - loss: 0.1444 - accuracy: 0.7973lr: 0.01\n",
      "3125/3125 [==============================] - 700s 224ms/step - loss: 0.1444 - accuracy: 0.7973 - val_loss: 0.1494 - val_accuracy: 0.7834\n",
      "Epoch 12/20\n",
      "3124/3125 [============================>.] - ETA: 0s - loss: 0.1417 - accuracy: 0.8017lr: 0.01\n",
      "3125/3125 [==============================] - 700s 224ms/step - loss: 0.1417 - accuracy: 0.8017 - val_loss: 0.1380 - val_accuracy: 0.8048\n",
      "Epoch 13/20\n",
      "3124/3125 [============================>.] - ETA: 0s - loss: 0.1395 - accuracy: 0.8058lr: 0.01\n",
      "3125/3125 [==============================] - 700s 224ms/step - loss: 0.1395 - accuracy: 0.8058 - val_loss: 0.1396 - val_accuracy: 0.8087\n",
      "Epoch 14/20\n",
      "3124/3125 [============================>.] - ETA: 0s - loss: 0.1388 - accuracy: 0.8069lr: 0.01\n",
      "3125/3125 [==============================] - 700s 224ms/step - loss: 0.1388 - accuracy: 0.8069 - val_loss: 0.1398 - val_accuracy: 0.8072\n",
      "Epoch 15/20\n",
      " 995/3125 [========>.....................] - ETA: 7:48 - loss: 0.1359 - accuracy: 0.8146"
     ]
    }
   ],
   "source": [
    "sd=[]\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = [1,1]\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        sd.append(step_decay(len(self.losses)))\n",
    "        print('lr:', step_decay(len(self.losses)))\n",
    "\n",
    "learning_rate = 0.01\n",
    "decay_rate = 5e-6\n",
    "momentum = 0.9\n",
    "batch_size_ = 32\n",
    "images_per_epoch_ = 100000\n",
    "epochs_ = 20\n",
    "\n",
    "sgd = keras.optimizers.SGD(lr = learning_rate,\n",
    "                           momentum = momentum, \n",
    "                           decay = decay_rate, \n",
    "                           nesterov=False)\n",
    "model.compile(loss = contrastive_loss, \n",
    "              optimizer=sgd, \n",
    "              metrics=[accuracy])\n",
    "\n",
    "def step_decay(losses):\n",
    "    if float(2 * np.sqrt(np.array(history.losses[-1]))) < 0.3:\n",
    "        lrate = 0.001 * 1 / (1 + 0.1 * len(history.losses))\n",
    "        momentum = 0.8\n",
    "        decay_rate = 2e-6\n",
    "        return lrate\n",
    "    else:\n",
    "        lrate = 0.01\n",
    "        return lrate\n",
    "\n",
    "history = LossHistory()\n",
    "lrate = keras.callbacks.LearningRateScheduler(step_decay)\n",
    "\n",
    "hist = model.fit(train_generator(train_dataframe, batch_size_), \n",
    "                 steps_per_epoch = images_per_epoch_ // batch_size_,\n",
    "                 epochs = epochs_,\n",
    "                 validation_data = test_generator(test_dataframe, batch_size_),\n",
    "                 validation_steps = len(test_dataframe) // batch_size_,\n",
    "                 callbacks = [history, lrate],\n",
    "                 shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = str(folder_path / 'sphereface_20.h5')\n",
    "model.save_weights(weights_path)\n",
    "# model.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict_generator(test_generator(test_dataframe, batch_size_), \n",
    "                               steps = len(test_dataframe) // batch_size_,\n",
    "                               verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
