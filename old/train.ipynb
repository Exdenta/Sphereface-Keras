{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-ef6eadc89237>:30: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%timeit\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # suppress Tensorflow verbose prints\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from utils import *\n",
    "from models import *\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import spatial\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import random\n",
    "import math\n",
    "import cv2\n",
    "\n",
    "models_path = Path.cwd() / 'models' / 'sphereface_20_keras'\n",
    "data_path = Path.cwd() / 'datasets'\n",
    "train_dataset_path = data_path / 'CASIA-WebFace-112x96'\n",
    "test_dataset_path = data_path / 'lfw_112x96'\n",
    "\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load test data\n",
      "load train data\n",
      "Wall time: 55.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"load test data\")\n",
    "test_dataframe = pd.read_csv(str(data_path / 'test.csv')) \n",
    "print(\"load train data\")\n",
    "train_dataframe = pd.read_csv(str(data_path / 'train.csv')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'fileL', 'fileR', 'flag'], dtype='object')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataframe.head()\n",
    "train_dataframe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>fileL</th>\n",
       "      <th>fileR</th>\n",
       "      <th>flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Abel_Pacheco\\Abel_Pacheco_0001.jpg</td>\n",
       "      <td>Abel_Pacheco\\Abel_Pacheco_0004.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Akhmed_Zakayev\\Akhmed_Zakayev_0001.jpg</td>\n",
       "      <td>Akhmed_Zakayev\\Akhmed_Zakayev_0003.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Akhmed_Zakayev\\Akhmed_Zakayev_0002.jpg</td>\n",
       "      <td>Akhmed_Zakayev\\Akhmed_Zakayev_0003.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Amber_Tamblyn\\Amber_Tamblyn_0001.jpg</td>\n",
       "      <td>Amber_Tamblyn\\Amber_Tamblyn_0002.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Angela_Bassett\\Angela_Bassett_0001.jpg</td>\n",
       "      <td>Angela_Bassett\\Angela_Bassett_0005.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                   fileL  \\\n",
       "0           0      Abel_Pacheco\\Abel_Pacheco_0001.jpg   \n",
       "1           1  Akhmed_Zakayev\\Akhmed_Zakayev_0001.jpg   \n",
       "2           2  Akhmed_Zakayev\\Akhmed_Zakayev_0002.jpg   \n",
       "3           3    Amber_Tamblyn\\Amber_Tamblyn_0001.jpg   \n",
       "4           4  Angela_Bassett\\Angela_Bassett_0001.jpg   \n",
       "\n",
       "                                    fileR  flag  \n",
       "0      Abel_Pacheco\\Abel_Pacheco_0004.jpg     1  \n",
       "1  Akhmed_Zakayev\\Akhmed_Zakayev_0003.jpg     1  \n",
       "2  Akhmed_Zakayev\\Akhmed_Zakayev_0003.jpg     1  \n",
       "3    Amber_Tamblyn\\Amber_Tamblyn_0002.jpg     1  \n",
       "4  Angela_Bassett\\Angela_Bassett_0005.jpg     1  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/how-to-load-large-datasets-from-directories-for-deep-learning-with-keras/\n",
    "# https://medium.com/@vijayabhaskar96/tutorial-on-keras-flow-from-dataframe-1fd4493d237c\n",
    "# https://stackoverflow.com/questions/49404993/keras-how-to-use-fit-generator-with-multiple-inputs\n",
    "\n",
    "def preprocess_image(image):\n",
    "    image = (image - 127.5) / 128\n",
    "    return image\n",
    "\n",
    "\n",
    "def train_generator(train_dataframe, batch_size_):\n",
    "    class_mode_ = \"binary\"\n",
    "    generator = ImageDataGenerator(preprocessing_function=preprocess_image, validation_split=0.0)\n",
    "    \n",
    "    train_generator_X1 = generator.flow_from_dataframe(\n",
    "                             dataframe=train_dataframe,\n",
    "                             directory=str(train_dataset_path) + \"/\",\n",
    "                             x_col=\"fileL\",\n",
    "                             y_col=\"flag\",\n",
    "                             subset=\"training\",\n",
    "                             batch_size=batch_size_,\n",
    "                             seed=42,\n",
    "                             shuffle=True,\n",
    "                             class_mode=class_mode_,\n",
    "                             color_mode='rgb',\n",
    "                             target_size=(112, 96))\n",
    "    \n",
    "    train_generator_X2 = generator.flow_from_dataframe(\n",
    "                             dataframe=train_dataframe,\n",
    "                             directory=str(train_dataset_path) + \"/\",\n",
    "                             x_col=\"fileR\",\n",
    "                             y_col=\"flag\",\n",
    "                             subset=\"training\",\n",
    "                             batch_size=batch_size_,\n",
    "                             seed=42,\n",
    "                             shuffle=True,\n",
    "                             class_mode=class_mode_,\n",
    "                             color_mode='rgb',\n",
    "                             target_size=(112, 96))\n",
    "    while True:\n",
    "        X1i = train_generator_X1.next()\n",
    "        X2i = train_generator_X2.next()\n",
    "        yield [X1i[0], X2i[0]], X1i[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generator(test_dataframe, batch_size):\n",
    "    class_mode_ = \"binary\"\n",
    "    test_datagen = ImageDataGenerator(\n",
    "                             preprocessing_function=preprocess_image) \n",
    "    test_generator_X1 = test_datagen.flow_from_dataframe(\n",
    "                             dataframe=test_dataframe,\n",
    "                             directory=str(test_dataset_path) + \"/\",\n",
    "                             x_col=\"fileL\",\n",
    "                             y_col=\"flag\",\n",
    "                             batch_size=batch_size,\n",
    "                             seed=42,\n",
    "                             shuffle=False,\n",
    "                             class_mode=class_mode_,\n",
    "                             color_mode='rgb',\n",
    "                             target_size=(112,96))\n",
    "\n",
    "    test_generator_X2 = test_datagen.flow_from_dataframe(\n",
    "                             dataframe=test_dataframe,\n",
    "                             directory=str(test_dataset_path) + \"/\",\n",
    "                             x_col=\"fileR\",\n",
    "                             y_col=\"flag\",\n",
    "                             batch_size=batch_size,\n",
    "                             seed=42,\n",
    "                             shuffle=False,\n",
    "                             class_mode=class_mode_,\n",
    "                             color_mode='rgb',\n",
    "                             target_size=(112,96))\n",
    "    while True:\n",
    "        X1i = test_generator_X1.next()\n",
    "        X2i = test_generator_X2.next()\n",
    "        yield [X1i[0], X2i[0]], X1i[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, Add, Activation, PReLU, Dense, Input, ZeroPadding2D, Lambda, GlobalAveragePooling2D, Dot \n",
    "from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.losses import CosineSimilarity\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "# from keras.engine.topology import Layer\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow import keras\n",
    "\n",
    "def conv_3_block(input, filters):\n",
    "    x = ZeroPadding2D(padding=(1, 1))(input)\n",
    "    x = Conv2D(filters, 3, strides=2, padding='valid', kernel_initializer='glorot_uniform')(x)\n",
    "    r1 = PReLU()(x)\n",
    "\n",
    "    x = ZeroPadding2D(padding=(1, 1))(r1)\n",
    "    x = Conv2D(filters, 3, strides=1, padding='valid', kernel_initializer=TruncatedNormal(stddev=0.01))(x)\n",
    "    r2 = PReLU()(x)\n",
    "\n",
    "    x = ZeroPadding2D(padding=(1, 1))(r2)\n",
    "    x = Conv2D(filters, 3, strides=1, padding='valid', kernel_initializer=TruncatedNormal(stddev=0.01))(x)\n",
    "    r3 = PReLU()(x)\n",
    "\n",
    "    x = Add()([r1, r3])\n",
    "    return x \n",
    "\n",
    "def conv_2_block(input, filters):\n",
    "    x = ZeroPadding2D(padding=(1, 1))(input)\n",
    "    x = Conv2D(filters, 3, strides=1, padding='valid', kernel_initializer=TruncatedNormal(stddev=0.01))(x)\n",
    "    x = PReLU()(x)\n",
    "\n",
    "    x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "    x = Conv2D(filters, 3, strides=1, padding='valid', kernel_initializer=TruncatedNormal(stddev=0.01))(x)\n",
    "    x = PReLU()(x)\n",
    "\n",
    "    x = Add()([input, x])\n",
    "    return x\n",
    "\n",
    "def sphereface20(input_shape):\n",
    "    input = Input(shape=input_shape)\n",
    "    x = conv_3_block(input, 64)\n",
    "    x = conv_3_block(x, 128)\n",
    "    x = conv_2_block(x, 128)\n",
    "    x = conv_3_block(x, 256)\n",
    "    x = conv_2_block(x, 256)\n",
    "    x = conv_2_block(x, 256)\n",
    "    x = conv_2_block(x, 256)\n",
    "    x = conv_3_block(x, 512)\n",
    "    \n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(512, kernel_initializer='glorot_uniform')(x)\n",
    "    \n",
    "    model = Model(input, x)\n",
    "    return model\n",
    "\n",
    "# https://keras.io/examples/mnist_siamese/\n",
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n",
    "    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n",
    "\n",
    "# https://stackoverflow.com/questions/51003027/computing-cosine-similarity-between-two-tensors-in-keras\n",
    "def cosine_distance(vests):\n",
    "    x, y = vests\n",
    "    x = K.l2_normalize(x, axis=-1)\n",
    "    y = K.l2_normalize(y, axis=-1)\n",
    "    return -K.mean(x * y, axis=-1, keepdims=True)\n",
    "\n",
    "def cos_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0],1)\n",
    "\n",
    "\n",
    "# network definition\n",
    "input_shape = (112, 96, 3)\n",
    "# base_network = sphereface20(input_shape)\n",
    "base_network = InceptionResNetV2(include_top=False, weights='imagenet', input_shape=input_shape,\n",
    "                                   pooling='avg')\n",
    "\n",
    "input_a = Input(shape=input_shape)\n",
    "input_b = Input(shape=input_shape)\n",
    "\n",
    "# because we re-use the same instance `base_network`,\n",
    "# the weights of the network\n",
    "# will be shared across the two branches\n",
    "processed_a = base_network(input_a)\n",
    "processed_b = base_network(input_b)\n",
    "distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([processed_a, processed_b])\n",
    "\n",
    "model = Model([input_a, input_b], distance)\n",
    "\n",
    "# serialize model to JSON\n",
    "folder_path = Path.cwd() / 'models' / 'sphereface_20_keras'\n",
    "folder_path.mkdir(parents=True, exist_ok=True)\n",
    "# model_path = str(folder_path / 'sphereface_20.json')\n",
    "# model_json = base_network.to_json()\n",
    "# with open(model_path, \"w\") as json_file:\n",
    "#     json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model FaceNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    return K.mean(K.equal(y_true, K.cast(y_pred < 0.5, y_true.dtype)))\n",
    "\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    '''Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    margin = 1\n",
    "    square_pred = K.square(y_pred)\n",
    "    margin_square = K.square(K.maximum(margin - y_pred, 0))\n",
    "    return K.mean(y_true * square_pred + (1 - y_true) * margin_square)\n",
    "\n",
    "def custom_loss(yTrue,yPred):\n",
    "    return K.sum(K.log(yTrue) - K.log(yPred))\n",
    "\n",
    "# # def cosine_similarity(y_true, y_pred):\n",
    "# #     y = tf.constant([c1,c2])\n",
    "# #     x = K.l2_normalize(y_true, -1)\n",
    "# #     y = K.l2_normalize(y_pred, -1)\n",
    "# #     s = K.mean(x * y, axis=-1, keepdims=False) * 10\n",
    "# #     return s\n",
    "\n",
    "# lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "#     0.01,\n",
    "#     decay_steps=10000,\n",
    "#     decay_rate=0.96,\n",
    "#     staircase=True)\n",
    "# # optimizer_ = keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "\n",
    "# # rms_ = RMSprop()\n",
    "# optimizer_ = RMSprop(learning_rate=0.001, rho=0.9, epsilon=1e-07)\n",
    "# # loss_ = custom_loss\n",
    "# # loss_ = CosineSimilarity(axis=-1, name='cosine_similarity')\n",
    "\n",
    "# model.compile(loss=contrastive_loss, optimizer=optimizer_, metrics=[accuracy])\n",
    "# # model.compile(loss=loss_, optimizer=rms_, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "rms = RMSprop()\n",
    "model.compile(loss=contrastive_loss, optimizer=rms, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd=[]\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = [1,1]\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        sd.append(step_decay(len(self.losses)))\n",
    "        print(', lr:', step_decay(len(self.losses)))\n",
    "\n",
    "learning_rate = 0.01\n",
    "decay_rate_ = 5e-6\n",
    "momentum_ = 0.9\n",
    "batch_size_ = 32\n",
    "images_per_epoch_ = 100000\n",
    "\n",
    "sgd = keras.optimizers.SGD(lr = learning_rate,\n",
    "                           momentum = momentum_, \n",
    "                           decay = decay_rate_, \n",
    "                           nesterov=False)\n",
    "model.compile(loss = contrastive_loss, \n",
    "              optimizer = sgd, \n",
    "              metrics = [accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_g = train_generator(train_dataframe, batch_size_)\n",
    "test_g = test_generator(test_dataframe, batch_size_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Found 46852992 validated image filenames belonging to 2 classes.\n",
      "Found 46852992 validated image filenames belonging to 2 classes.\n",
      "WARNING:tensorflow:From /home/paperspace/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "2552/3125 [=======================>......] - ETA: 8:06 - loss: 1.7943 - accuracy: 0.5228"
     ]
    }
   ],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    momentum=momentum_      # 0.8\n",
    "    decay_rate=decay_rate_  # 2e-6\n",
    "    lr = learning_rate      # 0.0005\n",
    "    return lr\n",
    "    \n",
    "def step_decay(losses):\n",
    "    lrate=learning_rate\n",
    "    momentum=momentum_\n",
    "    decay_rate=decay_rate_\n",
    "    return lrate\n",
    "\n",
    "history = LossHistory()\n",
    "lrate = keras.callbacks.LearningRateScheduler(scheduler)\n",
    "callbacks_ = [history, lrate]\n",
    "# callbacks_ = [history]\n",
    "initial_epoch_ = 0\n",
    "epochs_ = 10\n",
    "\n",
    "try:\n",
    "    hist = model.fit(train_g,\n",
    "                     steps_per_epoch=images_per_epoch_ // batch_size_,\n",
    "                     epochs=epochs_,\n",
    "                     validation_data=test_g,\n",
    "                     validation_steps=len(test_dataframe) // batch_size_,\n",
    "                     callbacks=callbacks_,\n",
    "                     initial_epoch=initial_epoch_,\n",
    "                     shuffle=True)\n",
    "except KeyboardInterrupt:\n",
    "    save_path = str(models_path / 'sphereface_20_interrupted.h5')\n",
    "    model.save_weights(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = str(models_path / 'sphereface_20_new.h5')\n",
    "model.save_weights(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = str(folder_path / 'sphereface_20_8372.h5')\n",
    "model.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict_generator(test_generator(test_dataframe, batch_size_), \n",
    "                               steps = len(test_dataframe) // batch_size_,\n",
    "                               verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
